{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN number: 1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "from functools import partial\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "\n",
    "import sys\n",
    "#RUN = int(sys.argv[1])\n",
    "RUN = 1\n",
    "print(\"RUN number:\", RUN)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import biogeme.database as db\n",
    "import seaborn as sns\n",
    "from scipy.special import softmax\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "import pyro.poutine as poutine\n",
    "from pyro.distributions.util import logsumexp\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, TraceEnum_ELBO\n",
    "from pyro.infer.abstract_infer import TracePredictive\n",
    "from pyro.contrib.autoguide import AutoMultivariateNormal, AutoDiagonalNormal, AutoGuideList\n",
    "from pyro.infer.mcmc import MCMC, NUTS\n",
    "import pyro.optim as optim\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import logging\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate fake data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating fake data...\n",
      "Error: 46.6\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "#Generate data\n",
    "###\n",
    "\n",
    "N = 500\n",
    "T = 5\n",
    "NT = N * T\n",
    "J = 5\n",
    "NTJ = NT * J\n",
    "\n",
    "L = 3 #no. of fixed paramters\n",
    "K = 5 #no. of random parameters\n",
    "\n",
    "true_alpha = np.array([-0.8, 0.8, 1.2])\n",
    "true_beta = np.array([-0.8, 0.8, 1.0, -0.8, 1.5])\n",
    "true_Omega = np.array([[1.0, 0.8, 0.8, 0.8, 0.8],\n",
    "                       [0.8, 1.0, 0.8, 0.8, 0.8],\n",
    "                       [0.8, 0.8, 1.0, 0.8, 0.8],\n",
    "                       [0.8, 0.8, 0.8, 1.0, 0.8],\n",
    "                       [0.8, 0.8, 0.8, 0.8, 1.0]])\n",
    "# dynamic version\n",
    "corr = 0.8\n",
    "scale_factor = 1.0\n",
    "true_Omega = corr*np.ones((K,K)) # off-diagonal values of cov matrix\n",
    "true_Omega[np.arange(K), np.arange(K)] = 1.0 # diagonal values of cov matrix\n",
    "true_Omega *= scale_factor\n",
    "\n",
    "print(\"Generating fake data...\")\n",
    "xFix = np.random.rand(NTJ, L)\n",
    "xRnd = np.random.rand(NTJ, K)\n",
    "\n",
    "betaInd_tmp = true_beta + \\\n",
    "(np.linalg.cholesky(true_Omega) @ np.random.randn(K, N)).T\n",
    "beta_tmp = np.kron(betaInd_tmp, np.ones((T * J,1)))\n",
    "\n",
    "eps = -np.log(-np.log(np.random.rand(NTJ,)))\n",
    "\n",
    "vDet = xFix @ true_alpha + np.sum(xRnd * beta_tmp, axis = 1)\n",
    "v = vDet + eps\n",
    "\n",
    "vDetMax = np.zeros((NT,))\n",
    "vMax = np.zeros((NT,))\n",
    "\n",
    "chosen = np.zeros((NTJ,), dtype = 'int64')\n",
    "\n",
    "for t in np.arange(NT):\n",
    "    l = t * J; u = (t + 1) * J\n",
    "    altMaxDet = np.argmax(vDet[l:u])\n",
    "    altMax = np.argmax(v[l:u])\n",
    "    vDetMax[t] = altMaxDet\n",
    "    vMax[t] = altMax\n",
    "    chosen[l + altMax] = 1\n",
    "\n",
    "error = np.sum(vMax == vDetMax) / NT * 100\n",
    "print(\"Error:\", error)\n",
    "\n",
    "indID = np.repeat(np.arange(N), T * J)\n",
    "obsID = np.repeat(np.arange(NT), J)\n",
    "altID = np.tile(np.arange(J), NT)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert fake data to wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_alternatives = altID.max() + 1\n",
    "num_resp = indID.max() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN == 1: # THIS IS SLOW!!! IF NOT CHANGED, IT IS FASTER TO READ THE PREVIOUS DATA FROM DISK\n",
    "    # convert long format to wide format\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for ind in range(num_resp):\n",
    "        #print(\"------------------ individual:\", ind)\n",
    "        ind_ix = np.where(indID == ind)[0]\n",
    "        #print(\"ind_ix:\", ind_ix)\n",
    "        ind_xs = []\n",
    "        ind_ys = []\n",
    "        for n in np.unique(obsID[ind_ix]):\n",
    "            #print(\"--------- observation:\", n)\n",
    "            obs_ix = np.where(obsID == n)[0]\n",
    "            #print(\"obs_ix:\", obs_ix)\n",
    "\n",
    "            # get attributes (x)\n",
    "            x = [[] for i in range(num_alternatives)]\n",
    "            #print(\"altID:\", altID[obs_ix])\n",
    "            for alt in range(num_alternatives):\n",
    "                if alt in altID[obs_ix]:\n",
    "                    x[alt].append(np.hstack([xFix[obs_ix][alt], xRnd[obs_ix][alt]]))\n",
    "                else:\n",
    "                    x[alt].append(np.zeros(L+K))\n",
    "            x = np.hstack(x)[0]\n",
    "            #print(\"x:\", x)\n",
    "            ind_xs.append(x)\n",
    "\n",
    "            # get choice (y)\n",
    "            y = np.argmax(chosen[obs_ix])\n",
    "            #print(\"y:\", y)\n",
    "            ind_ys.append(y)\n",
    "\n",
    "        xs.append(np.array(ind_xs))\n",
    "        ys.append(np.array(ind_ys))\n",
    "\n",
    "    alt_availability = np.ones((N,T,J))\n",
    "    alt_attributes = np.array(xs)\n",
    "    true_choices = np.array(ys)\n",
    "    \n",
    "    np.savez('fakedata.npz', \n",
    "             alt_availability=alt_availability, \n",
    "             alt_attributes=alt_attributes, \n",
    "             true_choices=true_choices)\n",
    "else:\n",
    "    # load previously generated data from disk\n",
    "    data = np.load('fakedata.npz')\n",
    "    alt_availability = data['alt_availability']\n",
    "    alt_attributes = data['alt_attributes']\n",
    "    true_choices = data['true_choices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alt. availability: (500, 5, 5)\n",
      "Alt. attributes: (500, 5, 40)\n",
      "True choices: (500, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"Alt. availability:\", alt_availability.shape)\n",
    "print(\"Alt. attributes:\", alt_attributes.shape)\n",
    "print(\"True choices:\", true_choices.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation and Mixed Logit specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. observations: 12500\n",
      "Num. alternatives: 5\n",
      "Parameter IDs to be treated in a Mixed Logit way: [3 4 5 6 7]\n",
      "Parameter IDs to be treated in a MNL way: [0 1 2]\n",
      "Utility functions:\n",
      "\tV_ALT1_n = beta0 * ALT1_XF1 + beta1 * ALT1_XF2 + beta2 * ALT1_XF3 + beta3_n * ALT1_XR1 + beta4_n * ALT1_XR2 + beta5_n * ALT1_XR3 + beta6_n * ALT1_XR4 + beta7_n * ALT1_XR5\n",
      "\tV_ALT2_n = beta0 * ALT2_XF1 + beta1 * ALT2_XF2 + beta2 * ALT2_XF3 + beta3_n * ALT2_XR1 + beta4_n * ALT2_XR2 + beta5_n * ALT2_XR3 + beta6_n * ALT2_XR4 + beta7_n * ALT2_XR5\n",
      "\tV_ALT3_n = beta0 * ALT3_XF1 + beta1 * ALT3_XF2 + beta2 * ALT3_XF3 + beta3_n * ALT3_XR1 + beta4_n * ALT3_XR2 + beta5_n * ALT3_XR3 + beta6_n * ALT3_XR4 + beta7_n * ALT3_XR5\n",
      "\tV_ALT4_n = beta0 * ALT4_XF1 + beta1 * ALT4_XF2 + beta2 * ALT4_XF3 + beta3_n * ALT4_XR1 + beta4_n * ALT4_XR2 + beta5_n * ALT4_XR3 + beta6_n * ALT4_XR4 + beta7_n * ALT4_XR5\n",
      "\tV_ALT5_n = beta0 * ALT5_XF1 + beta1 * ALT5_XF2 + beta2 * ALT5_XF3 + beta3_n * ALT5_XR1 + beta4_n * ALT5_XR2 + beta5_n * ALT5_XR3 + beta6_n * ALT5_XR4 + beta7_n * ALT5_XR5\n",
      "Num. parameters to be estimated: 8\n",
      "Num. attributes to be used in total: 40\n",
      "Num respondents: 500\n"
     ]
    }
   ],
   "source": [
    "# DCM specification\n",
    "num_obs = len(chosen)\n",
    "print(\"Num. observations:\", num_obs)\n",
    "\n",
    "alt_names = [\"ALT1\", \"ALT2\", \"ALT3\", \"ALT4\", \"ALT5\"]\n",
    "assert num_alternatives == len(alt_names)\n",
    "print(\"Num. alternatives:\", num_alternatives)\n",
    "\n",
    "attr_names = ['ALT1_XF1', 'ALT1_XF2','ALT1_XF3', 'ALT1_XR1', 'ALT1_XR2','ALT1_XR3', 'ALT1_XR4', 'ALT1_XR5', \n",
    "              'ALT2_XF1', 'ALT2_XF2','ALT2_XF3', 'ALT2_XR1', 'ALT2_XR2','ALT2_XR3', 'ALT2_XR4', 'ALT2_XR5', \n",
    "              'ALT3_XF1', 'ALT3_XF2','ALT3_XF3', 'ALT3_XR1', 'ALT3_XR2','ALT3_XR3', 'ALT3_XR4', 'ALT3_XR5', \n",
    "              'ALT4_XF1', 'ALT4_XF2','ALT4_XF3', 'ALT4_XR1', 'ALT4_XR2','ALT4_XR3', 'ALT4_XR4', 'ALT4_XR5', \n",
    "              'ALT5_XF1', 'ALT5_XF2','ALT5_XF3', 'ALT5_XR1', 'ALT5_XR2','ALT5_XR3', 'ALT5_XR4', 'ALT5_XR5', ] \n",
    "alt_ids = np.array([0,0,0,0,0,0,0,0,\n",
    "                    1,1,1,1,1,1,1,1,\n",
    "                    2,2,2,2,2,2,2,2,\n",
    "                    3,3,3,3,3,3,3,3,\n",
    "                    4,4,4,4,4,4,4,4]) # assigns attributes to IDs corresponding to alternatives\n",
    "param_ids = np.array([0,1,2,3,4,5,6,7,\n",
    "                      0,1,2,3,4,5,6,7,\n",
    "                      0,1,2,3,4,5,6,7,\n",
    "                      0,1,2,3,4,5,6,7,\n",
    "                      0,1,2,3,4,5,6,7]) # assigns attributes to IDs indicating parameters to be estimated\n",
    "mix_params = np.array([3,4,5,6,7]) # IDs of parameters to be treated with a Mixed Logit formulation\n",
    "non_mix_params = np.array([x for x in range(max(param_ids)+1) if x not in mix_params])\n",
    "print(\"Parameter IDs to be treated in a Mixed Logit way:\", mix_params)\n",
    "print(\"Parameter IDs to be treated in a MNL way:\", non_mix_params)\n",
    "\n",
    "# debug utility functions specified\n",
    "print(\"Utility functions:\")\n",
    "for i in range(num_alternatives):\n",
    "    v_ix = np.where(alt_ids == i)[0]\n",
    "    if param_ids[v_ix[0]] in mix_params:\n",
    "        s = \"\\tV_%s_n = beta%d_n * %s\" % (alt_names[i], param_ids[v_ix[0]], attr_names[v_ix[0]])\n",
    "    else:\n",
    "        s = \"\\tV_%s_n = beta%d * %s\" % (alt_names[i], param_ids[v_ix[0]], attr_names[v_ix[0]])\n",
    "    for j in range(1,len(v_ix)):\n",
    "        if param_ids[v_ix[j]] in mix_params:\n",
    "            s += \" + beta%d_n * %s\" % (param_ids[v_ix[j]], attr_names[v_ix[j]])\n",
    "        else:\n",
    "            s += \" + beta%d * %s\" % (param_ids[v_ix[j]], attr_names[v_ix[j]])\n",
    "    print(s)\n",
    "\n",
    "# further checks and definitions\n",
    "assert len(np.unique(param_ids)) == max(param_ids)+1\n",
    "assert min(param_ids) == 0\n",
    "num_params = max(param_ids) + 1\n",
    "print(\"Num. parameters to be estimated:\", num_params)\n",
    "D = len(attr_names)\n",
    "print(\"Num. attributes to be used in total:\", D)\n",
    "assert len(attr_names) == len(alt_ids) # length check\n",
    "assert max(alt_ids) + 1 == num_alternatives    \n",
    "\n",
    "resp_ids = np.arange(num_resp)\n",
    "print(\"Num respondents:\", num_resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Mixed Logit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auxiliary dictionary for Pyro model implementation\n",
    "beta_to_params_map = [param_ids[np.where(alt_ids == i)[0]] for i in range(num_alternatives)]\n",
    "\n",
    "# auxiliary CUDA matrix for Pyro model\n",
    "zeros_vec = torch.zeros(T,num_resp,num_alternatives).cuda()\n",
    "\n",
    "pyro.enable_validation(True)    # <---- This is always a good idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 500\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = num_resp \n",
    "#BATCH_SIZE = 2000 # CHANGED\n",
    "#BATCH_SIZE = int(num_resp / 5)\n",
    "print(\"Batch size:\", BATCH_SIZE)\n",
    "\n",
    "diagonal_alpha = False\n",
    "diagonal_beta_mu = False\n",
    "\n",
    "def model(x, y, alt_av, alt_ids_cuda):\n",
    "    # global parameters in the model\n",
    "    if diagonal_alpha:\n",
    "        alpha_mu = pyro.sample(\"alpha\", dist.Normal(torch.zeros(len(non_mix_params), device=x.device), 1).to_event(1))\n",
    "    else:\n",
    "        alpha_mu = pyro.sample(\"alpha\", dist.MultivariateNormal(torch.zeros(len(non_mix_params), device=x.device), \n",
    "                                            scale_tril=torch.tril(1*torch.eye(len(non_mix_params), device=x.device))))\n",
    "    \n",
    "    if diagonal_beta_mu:\n",
    "        beta_mu = pyro.sample(\"beta_mu\", dist.Normal(torch.zeros(len(mix_params), device=x.device), 1.).to_event(1))\n",
    "    else:\n",
    "        beta_mu = pyro.sample(\"beta_mu\", dist.MultivariateNormal(torch.zeros(len(mix_params), device=x.device), \n",
    "                                            scale_tril=torch.tril(1*torch.eye(len(mix_params), device=x.device))))\n",
    "    \n",
    "    beta_scale = 1./torch.sqrt(pyro.sample(\"beta_scale\", \n",
    "                                       dist.Gamma(.1, 1.*torch.ones(len(mix_params), device=x.device)).to_event(1)))\n",
    "    \n",
    "    # local parameters in the model\n",
    "    random_params = pyro.sample(\"beta_resp\", dist.Normal(beta_mu.repeat(num_resp,1), \n",
    "                                                         beta_scale.repeat(num_resp,1)).to_event(2))\n",
    "    \n",
    "    # vector of respondent parameters: global + local (respondent)\n",
    "    params_resp = torch.cat([alpha_mu.repeat(num_resp,1), random_params], dim=-1)\n",
    "\n",
    "    # vector of betas of MXL (may repeat the same learnable parameter multiple times; random + fixed effects)\n",
    "    beta_resp = torch.cat([params_resp[:,beta_to_params_map[i]] for i in range(num_alternatives)], dim=-1)\n",
    "    \n",
    "    with pyro.plate(\"locals\", len(x), subsample_size=BATCH_SIZE) as ind:\n",
    "        \n",
    "        with pyro.plate(\"data_resp\", T):\n",
    "            # compute utilities for each alternative\n",
    "            utilities = torch.scatter_add(zeros_vec[:,ind,:],\n",
    "                                          2, \n",
    "                                          alt_ids_cuda[ind,:,:].transpose(0,1), \n",
    "                                          torch.mul(x[ind,:,:].transpose(0,1), beta_resp[ind,:]))\n",
    "            \n",
    "            # adjust utility for unavailable alternatives\n",
    "            utilities += alt_av[ind,:,:].transpose(0,1)\n",
    "\n",
    "            # likelihood\n",
    "            pyro.sample(\"obs\", dist.Categorical(logits=utilities), obs=y[ind,:].transpose(0,1))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify variational approximation q (guide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel size: 50\n",
      "Predictor(\n",
      "  (cnn1): Conv1d(1, 200, kernel_size=(50,), stride=(50,))\n",
      "  (cnn2): Conv1d(200, 200, kernel_size=(1,), stride=(1,))\n",
      "  (bn): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=200, bias=True)\n",
      "  (fc3mu): Linear(in_features=200, out_features=5, bias=True)\n",
      "  (fc3sigma): Linear(in_features=200, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (pooling): MaxPool1d(kernel_size=5, stride=5, padding=0, dilation=1, ceil_mode=False)\n",
      "  (softplus): Softplus(beta=1, threshold=20)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "kernel_size = num_params*num_alternatives+num_alternatives*2\n",
    "print(\"Kernel size:\", kernel_size)\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    def __init__(self, z_dim, hidden_dim):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.cnn1 = torch.nn.Conv1d(1, 200, kernel_size=(kernel_size), stride=(kernel_size), \n",
    "                                    padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        self.cnn2 = torch.nn.Conv1d(200, 200, kernel_size=(1), stride=(1), \n",
    "                                    padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(1)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.fc1 = nn.Linear(200, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.fc3sigma = nn.Linear(hidden_dim, z_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        #self.pooling = nn.AvgPool1d(T, stride=(T))\n",
    "        self.pooling = nn.MaxPool1d(T, stride=(T))\n",
    "        \n",
    "        # setup the non-linearities\n",
    "        self.softplus = nn.Softplus()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute the hidden units\n",
    "        hidden = self.bn(x)\n",
    "        #hidden = x\n",
    "        hidden = self.cnn1(hidden)\n",
    "        hidden = self.relu(self.pooling(hidden))\n",
    "        hidden = self.bn2(hidden)\n",
    "        #hidden = hidden.flatten(1,2)\n",
    "        hidden = self.relu(self.fc1(hidden.flatten(1,2)))\n",
    "        \n",
    "        # return a mean vector and a (positive) square root covariance\n",
    "        # each of size batch_size x z_dim\n",
    "        z_loc = self.fc3mu(hidden)\n",
    "        z_scale = self.softplus(self.fc3sigma(hidden))\n",
    "        return z_loc, z_scale\n",
    "    \n",
    "predictor = Predictor(5, 200).cuda()\n",
    "print(predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters by layer: [10200, 40200, 2, 400, 1005, 1005]\n",
      "Total parameters: 52812\n"
     ]
    }
   ],
   "source": [
    "layers = [predictor.cnn1, predictor.fc1, predictor.bn, predictor.bn2, predictor.fc3mu, predictor.fc3sigma]\n",
    "pytorch_params = [sum(p.numel() for p in l.parameters() if p.requires_grad) for l in layers]\n",
    "print(\"Parameters by layer:\", pytorch_params)\n",
    "print(\"Total parameters:\", sum(pytorch_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_av_cuda = torch.from_numpy(alt_availability)\n",
    "alt_av_cuda = alt_av_cuda.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softplus\n",
    "\n",
    "def guide(x, y, alt_av, alt_ids):\n",
    "    if diagonal_alpha:\n",
    "        alpha_loc = pyro.param('alpha_loc', torch.randn(len(non_mix_params), device=x.device))\n",
    "        alpha_scale = pyro.param('alpha_scale', 1*torch.ones(len(non_mix_params), device=x.device),\n",
    "                                 constraint=constraints.positive)\n",
    "        alpha = pyro.sample(\"alpha\", dist.Normal(alpha_loc, alpha_scale).to_event(1))\n",
    "    else:\n",
    "        alpha_loc = pyro.param('alpha_loc', torch.randn(len(non_mix_params), device=x.device))\n",
    "        alpha_scale = pyro.param(\"alpha_scale\", torch.tril(1*torch.eye(len(non_mix_params), device=x.device)),\n",
    "                                 constraint=constraints.lower_cholesky)\n",
    "        alpha = pyro.sample(\"alpha\", dist.MultivariateNormal(alpha_loc, scale_tril=alpha_scale))\n",
    "    \n",
    "    if diagonal_beta_mu:\n",
    "        beta_mu_loc = pyro.param('beta_mu_loc', torch.randn(len(mix_params), device=x.device))\n",
    "        beta_mu_scale = pyro.param('beta_mu_scale', 1*torch.ones(len(mix_params), device=x.device),\n",
    "                                   constraint=constraints.positive)\n",
    "        beta_mu = pyro.sample(\"beta_mu\", dist.Normal(beta_mu_loc, beta_mu_scale).to_event(1))\n",
    "    else:\n",
    "        beta_mu_loc = pyro.param('beta_mu_loc', torch.randn(len(mix_params), device=x.device))\n",
    "        beta_mu_scale = pyro.param(\"beta_mu_scale\", torch.tril(1*torch.eye(len(mix_params), device=x.device)),\n",
    "                                   constraint=constraints.lower_cholesky)\n",
    "        beta_mu = pyro.sample(\"beta_mu\", dist.MultivariateNormal(beta_mu_loc, scale_tril=beta_mu_scale))\n",
    "    \n",
    "    #beta_scale_shape = softplus(pyro.param(\"beta_scale_shape\", 20.*torch.ones(len(mix_params), device=x.device)))\n",
    "    #beta_scale_rate = softplus(pyro.param(\"beta_scale_rate\", 20.*torch.ones(len(mix_params), device=x.device)))\n",
    "    beta_scale_shape = pyro.param('beta_scale_shape', 20.*torch.ones(len(mix_params), device=x.device),\n",
    "                                   constraint=constraints.positive)\n",
    "    beta_scale_rate = pyro.param('beta_scale_rate', 20.*torch.ones(len(mix_params), device=x.device),\n",
    "                                   constraint=constraints.positive)\n",
    "    beta_scale = pyro.sample(\"beta_scale\", dist.Gamma(beta_scale_shape, beta_scale_rate).to_event(1))\n",
    "    \n",
    "    # Use an amortized guide for local variables.\n",
    "    pyro.module(\"predictor\", predictor)\n",
    "    one_hot = torch.zeros(num_resp, T, num_alternatives, device=x.device, dtype=torch.float)\n",
    "    one_hot = one_hot.scatter(2, y.unsqueeze(2).long(), 1)\n",
    "    inference_data = torch.cat([one_hot, x, alt_av_cuda.float()], dim=-1)\n",
    "    beta_loc, beta_scale = predictor.forward(inference_data.flatten(1,2).unsqueeze(1))\n",
    "    pyro.sample(\"beta_resp\", dist.Normal(beta_loc, beta_scale).to_event(2))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare data for running inference\n",
    "train_x = torch.tensor(alt_attributes, dtype=torch.float)\n",
    "train_x = train_x.cuda()\n",
    "train_y = torch.tensor(true_choices, dtype=torch.int)\n",
    "train_y = train_y.cuda()\n",
    "alt_av_cuda = torch.from_numpy(alt_availability)\n",
    "alt_av_cuda = alt_av_cuda.cuda()\n",
    "alt_av_mat = alt_availability.copy()\n",
    "alt_av_mat[np.where(alt_av_mat == 0)] = -1e9\n",
    "alt_av_mat -= 1\n",
    "alt_av_mat_cuda = torch.from_numpy(alt_av_mat).float()\n",
    "alt_av_mat_cuda = alt_av_mat_cuda.cuda()\n",
    "#alt_ids_cuda = torch.from_numpy(alt_ids[:,np.newaxis].repeat(1*num_resp,1).T.reshape(num_resp,1,-1))\n",
    "alt_ids_cuda = torch.from_numpy(alt_ids[:,np.newaxis].repeat(T*num_resp,1).T.reshape(num_resp,T,-1))\n",
    "alt_ids_cuda = alt_ids_cuda.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = torch.zeros(num_resp, T, num_alternatives, device=train_x.device, dtype=torch.float)\n",
    "one_hot = one_hot.scatter(2, train_y.unsqueeze(2).long(), 1)\n",
    "inference_data = torch.cat([one_hot, train_x, alt_av_cuda.float()], dim=-1)\n",
    "beta_loc, beta_scale = predictor.forward(inference_data.flatten(1,2).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#trace = poutine.trace(model).get_trace(train_x, train_y, alt_av_mat_cuda, alt_ids_cuda)\n",
    "#trace.compute_log_prob()  # optional, but allows printing of log_prob shapes\n",
    "#print(trace.format_shapes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "# function for calculating likelihood and accuracy\n",
    "def loglikelihood(X, y, alt_av, alpha, beta, beta_resps):\n",
    "    # gather vector of params for respondent\n",
    "    params_resp = np.hstack([alpha[:,np.newaxis].repeat(num_resp,1).T, beta_resps])\n",
    "    \n",
    "    # build vector of betas for respondent\n",
    "    beta_resp = np.hstack([params_resp[:,param_ids[np.where(alt_ids == i)[0]]] for i in range(num_alternatives)])\n",
    "    \n",
    "    # calculate utilities based on params\n",
    "    utilities = np.zeros((num_resp, T, J))\n",
    "    for resp_id in range(num_resp):\n",
    "        for i in range(num_alternatives):\n",
    "            utilities[resp_id,:,i] = np.dot(X[resp_id,:,np.where(alt_ids == i)[0]].T, \n",
    "                                            beta_resp[resp_id, np.where(alt_ids == i)[0]]).T\n",
    "\n",
    "    # adjust utility for unavailable alternatives\n",
    "    utilities += alt_av\n",
    "\n",
    "    # likelihood\n",
    "    probs = softmax(utilities, axis=2)\n",
    "    loglik = np.sum(np.log(probs.reshape(num_resp*T,J)[np.arange(num_resp*T), y.flatten()]))\n",
    "    acc = np.mean(np.argmax(probs, axis=2) == y[:,:])\n",
    "    \n",
    "    return loglik, acc\n",
    "\n",
    "def sim_loglikelihood(X, y, alt_av, alpha, beta, betaCovChol, num_samples=1000):\n",
    "    #betaCovChol = np.linalg.cholesky(betaCov)\n",
    "    pSim = np.zeros((num_samples, num_resp))\n",
    "\n",
    "    for i in np.arange(num_samples):\n",
    "        paramRnd = beta + (betaCovChol @ np.random.randn(K, num_resp)).T\n",
    "\n",
    "        # gather vector of params for respondent\n",
    "        params_resp = np.hstack([alpha[:,np.newaxis].repeat(num_resp,1).T, paramRnd])\n",
    "\n",
    "        # build vector of betas for respondent\n",
    "        beta_resp = np.hstack([params_resp[:, param_ids[np.where(alt_ids == i)[0]]] for i in range(num_alternatives)])\n",
    "\n",
    "        for resp_id in range(num_resp):\n",
    "            # calculate utilities based on params\n",
    "            utilities = np.vstack([np.dot(X[resp_id,:,np.where(alt_ids == i)[0]].T, \n",
    "                                          beta_resp[resp_id, np.where(alt_ids == i)[0]]) for i in range(num_alternatives)])\n",
    "\n",
    "            # adjust utility for unavailable alternatives\n",
    "            utilities = utilities.T + alt_av[resp_id]\n",
    "\n",
    "            # likelihood\n",
    "            probs = softmax(utilities, axis=1)\n",
    "            lPInd = np.sum(np.log(probs[np.arange(T), y[resp_id]]))\n",
    "\n",
    "            pSim[i, resp_id] = np.exp(lPInd)\n",
    "\n",
    "    logLik = np.sum(np.log(np.mean(pSim, axis=0)))\n",
    "    \n",
    "    return logLik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_param_args(module_name, param_name):\n",
    "    if 'predictor' in module_name:\n",
    "        return {\"lr\": 0.0005}\n",
    "    elif '_loc' in param_name:\n",
    "        return {\"lr\": 0.005}\n",
    "    elif '_scale' in param_name:\n",
    "        return {\"lr\": 0.005}\n",
    "    else:\n",
    "        raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rodr/env36/lib/python3.6/site-packages/pyro/infer/svi.py:50: FutureWarning: The `num_samples` argument to SVI is deprecated and will be removed in a future release. Use `pyro.infer.Predictive` class to draw samples from the posterior.\n",
      "  'samples from the posterior.', FutureWarning)\n",
      "[Epoch 0] Elbo: 8327; Loglik: -4130; Acc.: 0.181; Alpha RMSE: 1.176; Beta RMSE: 2.027; BetaInd RMSE: 1.423\n",
      "[Epoch 100] Elbo: 6902; Loglik: -3854; Acc.: 0.302; Alpha RMSE: 0.862; Beta RMSE: 1.782; BetaInd RMSE: 1.593\n",
      "[Epoch 200] Elbo: 4669; Loglik: -3336; Acc.: 0.482; Alpha RMSE: 0.608; Beta RMSE: 1.535; BetaInd RMSE: 1.354\n",
      "[Epoch 300] Elbo: 4218; Loglik: -3076; Acc.: 0.560; Alpha RMSE: 0.397; Beta RMSE: 1.254; BetaInd RMSE: 1.203\n",
      "[Epoch 400] Elbo: 4027; Loglik: -2932; Acc.: 0.594; Alpha RMSE: 0.289; Beta RMSE: 0.996; BetaInd RMSE: 1.095\n",
      "[Epoch 500] Elbo: 3813; Loglik: -2842; Acc.: 0.597; Alpha RMSE: 0.188; Beta RMSE: 0.738; BetaInd RMSE: 1.023\n",
      "[Epoch 600] Elbo: 3762; Loglik: -2827; Acc.: 0.591; Alpha RMSE: 0.126; Beta RMSE: 0.501; BetaInd RMSE: 0.967\n",
      "[Epoch 700] Elbo: 3725; Loglik: -2832; Acc.: 0.576; Alpha RMSE: 0.098; Beta RMSE: 0.334; BetaInd RMSE: 0.935\n",
      "[Epoch 800] Elbo: 3645; Loglik: -2830; Acc.: 0.573; Alpha RMSE: 0.102; Beta RMSE: 0.200; BetaInd RMSE: 0.920\n",
      "[Epoch 900] Elbo: 3679; Loglik: -2863; Acc.: 0.557; Alpha RMSE: 0.101; Beta RMSE: 0.106; BetaInd RMSE: 0.908\n",
      "[Epoch 1000] Elbo: 3650; Loglik: -2878; Acc.: 0.555; Alpha RMSE: 0.105; Beta RMSE: 0.054; BetaInd RMSE: 0.898\n",
      "[Epoch 1100] Elbo: 3646; Loglik: -2900; Acc.: 0.551; Alpha RMSE: 0.098; Beta RMSE: 0.063; BetaInd RMSE: 0.900\n",
      "[Epoch 1200] Elbo: 3628; Loglik: -2909; Acc.: 0.546; Alpha RMSE: 0.084; Beta RMSE: 0.070; BetaInd RMSE: 0.900\n",
      "[Epoch 1300] Elbo: 3626; Loglik: -2939; Acc.: 0.544; Alpha RMSE: 0.099; Beta RMSE: 0.084; BetaInd RMSE: 0.894\n",
      "[Epoch 1400] Elbo: 3681; Loglik: -2931; Acc.: 0.544; Alpha RMSE: 0.098; Beta RMSE: 0.089; BetaInd RMSE: 0.894\n",
      "[Epoch 1500] Elbo: 3610; Loglik: -2975; Acc.: 0.538; Alpha RMSE: 0.110; Beta RMSE: 0.090; BetaInd RMSE: 0.893\n",
      "[Epoch 1600] Elbo: 3632; Loglik: -2988; Acc.: 0.534; Alpha RMSE: 0.112; Beta RMSE: 0.095; BetaInd RMSE: 0.892\n",
      "[Epoch 1700] Elbo: 3618; Loglik: -2983; Acc.: 0.536; Alpha RMSE: 0.107; Beta RMSE: 0.089; BetaInd RMSE: 0.888\n",
      "[Epoch 1800] Elbo: 3617; Loglik: -2989; Acc.: 0.531; Alpha RMSE: 0.102; Beta RMSE: 0.081; BetaInd RMSE: 0.888\n",
      "[Epoch 1900] Elbo: 3654; Loglik: -3001; Acc.: 0.530; Alpha RMSE: 0.101; Beta RMSE: 0.085; BetaInd RMSE: 0.884\n",
      "[Epoch 2000] Elbo: 3624; Loglik: -3005; Acc.: 0.524; Alpha RMSE: 0.122; Beta RMSE: 0.072; BetaInd RMSE: 0.887\n",
      "[Epoch 2100] Elbo: 3615; Loglik: -3018; Acc.: 0.522; Alpha RMSE: 0.114; Beta RMSE: 0.071; BetaInd RMSE: 0.886\n",
      "[Epoch 2200] Elbo: 3616; Loglik: -3014; Acc.: 0.525; Alpha RMSE: 0.094; Beta RMSE: 0.080; BetaInd RMSE: 0.883\n",
      "[Epoch 2300] Elbo: 3642; Loglik: -3023; Acc.: 0.526; Alpha RMSE: 0.105; Beta RMSE: 0.079; BetaInd RMSE: 0.882\n",
      "[Epoch 2400] Elbo: 3642; Loglik: -3027; Acc.: 0.521; Alpha RMSE: 0.097; Beta RMSE: 0.070; BetaInd RMSE: 0.885\n",
      "[Epoch 2500] Elbo: 3628; Loglik: -3037; Acc.: 0.523; Alpha RMSE: 0.105; Beta RMSE: 0.077; BetaInd RMSE: 0.886\n",
      "[Epoch 2600] Elbo: 3659; Loglik: -3032; Acc.: 0.524; Alpha RMSE: 0.106; Beta RMSE: 0.072; BetaInd RMSE: 0.886\n",
      "[Epoch 2700] Elbo: 3625; Loglik: -3041; Acc.: 0.520; Alpha RMSE: 0.121; Beta RMSE: 0.067; BetaInd RMSE: 0.887\n",
      "[Epoch 2800] Elbo: 3638; Loglik: -3054; Acc.: 0.520; Alpha RMSE: 0.108; Beta RMSE: 0.086; BetaInd RMSE: 0.887\n",
      "[Epoch 2900] Elbo: 3622; Loglik: -3056; Acc.: 0.518; Alpha RMSE: 0.133; Beta RMSE: 0.086; BetaInd RMSE: 0.883\n",
      "[Epoch 3000] Elbo: 3612; Loglik: -3066; Acc.: 0.515; Alpha RMSE: 0.121; Beta RMSE: 0.081; BetaInd RMSE: 0.882\n",
      "[Epoch 3100] Elbo: 3604; Loglik: -3070; Acc.: 0.512; Alpha RMSE: 0.113; Beta RMSE: 0.083; BetaInd RMSE: 0.885\n",
      "[Epoch 3200] Elbo: 3620; Loglik: -3072; Acc.: 0.511; Alpha RMSE: 0.125; Beta RMSE: 0.085; BetaInd RMSE: 0.885\n",
      "[Epoch 3300] Elbo: 3606; Loglik: -3066; Acc.: 0.516; Alpha RMSE: 0.094; Beta RMSE: 0.092; BetaInd RMSE: 0.885\n",
      "[Epoch 3400] Elbo: 3665; Loglik: -3073; Acc.: 0.516; Alpha RMSE: 0.129; Beta RMSE: 0.070; BetaInd RMSE: 0.887\n",
      "[Epoch 3500] Elbo: 3608; Loglik: -3076; Acc.: 0.513; Alpha RMSE: 0.125; Beta RMSE: 0.076; BetaInd RMSE: 0.887\n",
      "[Epoch 3600] Elbo: 3598; Loglik: -3074; Acc.: 0.508; Alpha RMSE: 0.104; Beta RMSE: 0.083; BetaInd RMSE: 0.886\n",
      "[Epoch 3700] Elbo: 3622; Loglik: -3086; Acc.: 0.511; Alpha RMSE: 0.112; Beta RMSE: 0.081; BetaInd RMSE: 0.887\n",
      "[Epoch 3800] Elbo: 3624; Loglik: -3088; Acc.: 0.514; Alpha RMSE: 0.124; Beta RMSE: 0.085; BetaInd RMSE: 0.887\n",
      "[Epoch 3900] Elbo: 3607; Loglik: -3089; Acc.: 0.510; Alpha RMSE: 0.116; Beta RMSE: 0.082; BetaInd RMSE: 0.886\n",
      "[Epoch 4000] Elbo: 3635; Loglik: -3090; Acc.: 0.510; Alpha RMSE: 0.125; Beta RMSE: 0.074; BetaInd RMSE: 0.886\n",
      "[Epoch 4100] Elbo: 3630; Loglik: -3088; Acc.: 0.511; Alpha RMSE: 0.138; Beta RMSE: 0.072; BetaInd RMSE: 0.886\n",
      "[Epoch 4200] Elbo: 3685; Loglik: -3094; Acc.: 0.510; Alpha RMSE: 0.130; Beta RMSE: 0.065; BetaInd RMSE: 0.886\n",
      "[Epoch 4300] Elbo: 3598; Loglik: -3100; Acc.: 0.506; Alpha RMSE: 0.125; Beta RMSE: 0.076; BetaInd RMSE: 0.883\n",
      "[Epoch 4400] Elbo: 3608; Loglik: -3088; Acc.: 0.514; Alpha RMSE: 0.116; Beta RMSE: 0.071; BetaInd RMSE: 0.882\n",
      "[Epoch 4500] Elbo: 3653; Loglik: -3096; Acc.: 0.510; Alpha RMSE: 0.130; Beta RMSE: 0.084; BetaInd RMSE: 0.887\n",
      "[Epoch 4600] Elbo: 3594; Loglik: -3095; Acc.: 0.508; Alpha RMSE: 0.129; Beta RMSE: 0.070; BetaInd RMSE: 0.882\n",
      "[Epoch 4700] Elbo: 3592; Loglik: -3092; Acc.: 0.511; Alpha RMSE: 0.117; Beta RMSE: 0.068; BetaInd RMSE: 0.881\n",
      "[Epoch 4800] Elbo: 3639; Loglik: -3101; Acc.: 0.509; Alpha RMSE: 0.126; Beta RMSE: 0.067; BetaInd RMSE: 0.883\n",
      "[Epoch 4900] Elbo: 3688; Loglik: -3094; Acc.: 0.505; Alpha RMSE: 0.130; Beta RMSE: 0.071; BetaInd RMSE: 0.883\n",
      "[Epoch 5000] Elbo: 3612; Loglik: -3100; Acc.: 0.505; Alpha RMSE: 0.123; Beta RMSE: 0.077; BetaInd RMSE: 0.884\n",
      "[Epoch 5100] Elbo: 3601; Loglik: -3094; Acc.: 0.506; Alpha RMSE: 0.132; Beta RMSE: 0.081; BetaInd RMSE: 0.882\n",
      "[Epoch 5200] Elbo: 3603; Loglik: -3106; Acc.: 0.506; Alpha RMSE: 0.125; Beta RMSE: 0.077; BetaInd RMSE: 0.886\n",
      "[Epoch 5300] Elbo: 3665; Loglik: -3094; Acc.: 0.511; Alpha RMSE: 0.107; Beta RMSE: 0.083; BetaInd RMSE: 0.888\n",
      "[Epoch 5400] Elbo: 3610; Loglik: -3100; Acc.: 0.511; Alpha RMSE: 0.142; Beta RMSE: 0.082; BetaInd RMSE: 0.885\n",
      "[Epoch 5500] Elbo: 3631; Loglik: -3104; Acc.: 0.509; Alpha RMSE: 0.131; Beta RMSE: 0.081; BetaInd RMSE: 0.885\n",
      "[Epoch 5600] Elbo: 3611; Loglik: -3097; Acc.: 0.506; Alpha RMSE: 0.117; Beta RMSE: 0.077; BetaInd RMSE: 0.886\n",
      "[Epoch 5700] Elbo: 3616; Loglik: -3103; Acc.: 0.509; Alpha RMSE: 0.138; Beta RMSE: 0.081; BetaInd RMSE: 0.885\n",
      "Elbo converged!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 128.77172946929932\n"
     ]
    }
   ],
   "source": [
    "svi = SVI(model,\n",
    "          guide,\n",
    "          #optim.ClippedAdam({\"lr\": 0.005}),\n",
    "          optim.ClippedAdam(per_param_args),\n",
    "          loss=Trace_ELBO(),\n",
    "          num_samples=1000)\n",
    "pyro.clear_param_store()\n",
    "    \n",
    "num_epochs = 10000\n",
    "track_loglik = True\n",
    "elbo_losses = []\n",
    "alpha_errors = []\n",
    "beta_errors = []\n",
    "betaInd_errors = []\n",
    "best_elbo = np.inf\n",
    "patience_thre = 10\n",
    "patience_count = 0\n",
    "tic = time.time()\n",
    "for j in range(num_epochs):\n",
    "    elbo = svi.step(train_x, train_y, alt_av_mat_cuda, alt_ids_cuda)\n",
    "    elbo_losses += [elbo]\n",
    "    \n",
    "    if j % 100 == 0:\n",
    "        if track_loglik:\n",
    "            alpha_params = pyro.param(\"alpha_loc\").data.cpu().numpy()\n",
    "            beta_params = pyro.param(\"beta_mu_loc\").data.cpu().numpy()\n",
    "            \n",
    "            beta_loc, beta_scale = predictor.forward(inference_data.flatten(1,2).unsqueeze(1))\n",
    "            params_resps = beta_loc.detach().cpu().numpy()\n",
    "            \n",
    "            alpha_rmse = np.sqrt(np.mean((true_alpha - alpha_params)**2))\n",
    "            beta_rmse = np.sqrt(np.mean((true_beta - beta_params)**2))\n",
    "            params_resps_rmse = np.sqrt(np.mean((betaInd_tmp - params_resps)**2))\n",
    "            alpha_errors += [alpha_rmse]\n",
    "            beta_errors += [beta_rmse]\n",
    "            betaInd_errors += [params_resps_rmse]\n",
    "            \n",
    "            loglik, acc = loglikelihood(alt_attributes, true_choices, alt_av_mat, \n",
    "                                        alpha_params, beta_params, params_resps)\n",
    "            logging.info(\"[Epoch %d] Elbo: %.0f; Loglik: %.0f; Acc.: %.3f; Alpha RMSE: %.3f; Beta RMSE: %.3f; BetaInd RMSE: %.3f\" % (j, \n",
    "                                                                          elbo, loglik, acc, alpha_rmse, beta_rmse, params_resps_rmse))\n",
    "        else:\n",
    "            logging.info(\"Elbo loss: %.2f\" % (elbo,))\n",
    "            \n",
    "        if np.mean(elbo_losses[-1000::10]) < best_elbo:\n",
    "            best_elbo = np.mean(elbo_losses[-1000::10])\n",
    "            patience_count = 0\n",
    "        else:\n",
    "            patience_count += 1\n",
    "            if patience_count >= patience_thre:\n",
    "                logging.info(\"Elbo converged!\")\n",
    "                break\n",
    "            \n",
    "toc = time.time() - tic\n",
    "print(\"Elapsed time:\", toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "alpha_params = pyro.param(\"alpha_loc\").data.cpu().numpy()\n",
    "beta_params = pyro.param(\"beta_mu_loc\").data.cpu().numpy()\n",
    "beta_params_cov = pyro.param(\"beta_mu_scale\").data.cpu().numpy()\n",
    "            \n",
    "beta_loc, beta_scale = predictor.forward(inference_data.flatten(1,2).unsqueeze(1))\n",
    "params_resps = beta_loc.detach().cpu().numpy()\n",
    "\n",
    "alpha_error = np.abs(true_alpha - alpha_params).mean()\n",
    "alpha_rmse = np.sqrt(np.mean((true_alpha - alpha_params)**2))\n",
    "beta_error = np.abs(true_beta - beta_params).mean()\n",
    "beta_rmse = np.sqrt(np.mean((true_beta - beta_params)**2))\n",
    "params_resps_error = np.abs(betaInd_tmp - params_resps).mean()\n",
    "params_resps_rmse = np.sqrt(np.mean((betaInd_tmp - params_resps)**2))\n",
    "\n",
    "loglik, acc = loglikelihood(alt_attributes, true_choices, alt_av_mat, \n",
    "                            alpha_params, beta_params, params_resps)\n",
    "\n",
    "loglik_hyp,_ = loglikelihood(alt_attributes, true_choices, alt_av_mat, \n",
    "                             alpha_params, beta_params, np.tile(beta_params, [N,T]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_scale_shape: [19.075 24.245 22.419 19.167 20.232]\n",
      "beta_scale_rate: [17.781 13.838 15.065 17.743 17.04 ]\n",
      "E[beta_scale]: [1.073 1.752 1.488 1.08  1.187]\n"
     ]
    }
   ],
   "source": [
    "beta_scale_shape = pyro.param(\"beta_scale_shape\").data.cpu().numpy()\n",
    "print(\"beta_scale_shape:\", beta_scale_shape)\n",
    "beta_scale_rate = pyro.param(\"beta_scale_rate\").data.cpu().numpy()\n",
    "print(\"beta_scale_rate:\", beta_scale_rate)\n",
    "print(\"E[beta_scale]:\", beta_scale_shape/beta_scale_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Omega_params = np.diag(beta_scale_shape/beta_scale_rate)\n",
    "Omega_rmse = np.sqrt(np.mean((true_Omega - Omega_params)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True alpha: [-0.8  0.8  1.2]\n",
      "Estimated alpha: [-0.823  0.72   0.976]\n",
      "Mean error (alpha): 0.10918842156728108\n",
      "RMSE (alpha): 0.13811397380226487\n",
      "\n",
      "True beta: [-0.8  0.8  1.  -0.8  1.5]\n",
      "Estimated beta: [-0.712  0.673  1.033 -0.803  1.586]\n",
      "Mean error (beta): 0.06748168230056764\n",
      "RMSE (beta): 0.08058641778088065\n",
      "\n",
      "True Omega: [[1.  0.8 0.8 0.8 0.8]\n",
      " [0.8 1.  0.8 0.8 0.8]\n",
      " [0.8 0.8 1.  0.8 0.8]\n",
      " [0.8 0.8 0.8 1.  0.8]\n",
      " [0.8 0.8 0.8 0.8 1. ]]\n",
      "Estimated Omega: [[1.073 0.    0.    0.    0.   ]\n",
      " [0.    1.752 0.    0.    0.   ]\n",
      " [0.    0.    1.488 0.    0.   ]\n",
      " [0.    0.    0.    1.08  0.   ]\n",
      " [0.    0.    0.    0.    1.187]]\n",
      "RMSE (Omega): 0.7389355512632477\n",
      "\n",
      "Mean error (params resps): 0.7039748437037466\n",
      "RMSE (params resps): 0.885359761897031\n",
      "\n",
      "Loglikelihood: -3102.579943526021\n",
      "\n",
      "Loglikelihood (hyper-priors only): -3553.4412958628427\n",
      "\n",
      "Loglikelihood (simulated at posterior means): -3545.5234874214048\n"
     ]
    }
   ],
   "source": [
    "print(\"True alpha:\", true_alpha)\n",
    "print(\"Estimated alpha:\", alpha_params)\n",
    "print(\"Mean error (alpha):\", alpha_error)\n",
    "print(\"RMSE (alpha):\", alpha_rmse)\n",
    "print(\"\\nTrue beta:\", true_beta)\n",
    "print(\"Estimated beta:\", beta_params)\n",
    "print(\"Mean error (beta):\", beta_error)\n",
    "print(\"RMSE (beta):\", beta_rmse)\n",
    "print(\"\\nTrue Omega:\", true_Omega)\n",
    "print(\"Estimated Omega:\", Omega_params)\n",
    "print(\"RMSE (Omega):\", Omega_rmse)\n",
    "print(\"\\nMean error (params resps):\", params_resps_error)\n",
    "print(\"RMSE (params resps):\", params_resps_rmse)\n",
    "print(\"\\nLoglikelihood:\", loglik)\n",
    "print(\"\\nLoglikelihood (hyper-priors only):\", loglik_hyp)\n",
    "\n",
    "sim_loglik = sim_loglikelihood(alt_attributes, true_choices, alt_av_mat, \n",
    "                               alpha_params, beta_params, np.linalg.cholesky(Omega_params), num_samples=200)\n",
    "print(\"\\nLoglikelihood (simulated at posterior means):\", sim_loglik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"Results_FakeData_N%d_T%d_J%d_L%d_K%d_Corr%.1f_Scale%.1f_Batch%d\" % (N,T,J,L,K,\n",
    "                                                                                   corr,scale_factor,\n",
    "                                                                                   BATCH_SIZE)\n",
    "\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "fname = output_dir + \"/Pyro_InvGamma_AmortizedVI.txt\"\n",
    "if not os.path.exists(fname):\n",
    "    fw = open(fname, \"w\")\n",
    "    fw.write(\"Run\\tTime\\tLoglik\\tSim. Loglik\\tLoglik (hyper)\\tRMSE alpha\\tRMSE beta\\tRMSE betaInd\\tRMSE Omega\\n\")\n",
    "else:\n",
    "    fw = open(fname, \"a\")\n",
    "    \n",
    "fw.write(\"%d\\t%.0f\\t%.1f\\t%.1f\\t%.1f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\n\" % (RUN, toc, \n",
    "                                                            loglik, sim_loglik, loglik_hyp, \n",
    "                                                            alpha_rmse, beta_rmse, params_resps_rmse, Omega_rmse))\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(fname.replace(\".txt\",\"_Run%d.pickle\" % (RUN,)), 'wb') as f:\n",
    "    pickle.dump({\"elbo_losses\": elbo_losses,\n",
    "                 \"alpha_errors\": alpha_errors,\n",
    "                 \"beta_errors\": beta_errors,\n",
    "                 \"betaInd_errors\": betaInd_errors}, \n",
    "                f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
